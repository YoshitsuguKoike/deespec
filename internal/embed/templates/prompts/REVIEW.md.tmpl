# Code Review Task

## **CRITICAL: Output File Path**

You MUST write your review to the EXACT path below. DO NOT create files anywhere else.

**Required Output Path**: `{{.ArtifactPath}}`

### **PATH VALIDATION**
- ✅ CORRECT: `.deespec/specs/sbi/{SBI_ID}/review_{turn}.md`
- ❌ WRONG: Any file in project root (e.g., `review_sbi_*.md`)
- ❌ WRONG: Any other directory (e.g., `.deespec/artifacts/`, `.deespec/runs/`, `.deespec/tasks/`)

**CRITICAL**: Use the Write tool with the EXACT path `{{.ArtifactPath}}` - do not modify or construct your own path.

### **Required File Template (COPY THIS STRUCTURE)**

```markdown
## Summary
DECISION: SUCCEEDED

[Brief summary in Japanese: implementation quality, issues found, test results]

## Review Details
[Detailed review content in Japanese...]

## Test Results
[Test execution results...]

## Recommendations
[If any improvements needed...]

{"decision": "succeeded"}
```

### **Validation Checklist**
After writing the file, verify:
- [ ] `## Summary` section exists in first 10 lines
- [ ] `DECISION: [VALUE]` appears immediately after `## Summary` heading (within 1-2 lines)
- [ ] JSON line `{"decision": "[value]"}` exists as the last line of the file
- [ ] Both DECISION values match (case-insensitive: SUCCEEDED = succeeded)

**IMPORTANT**: If both metadata match, the system reads the decision from the file. If mismatch or missing, it falls back to agent output parsing.

---

## Context
- Working Directory: `{{.WorkDir}}`
- SBI ID: {{.SBIID}}
- Turn: {{.Turn}}
- Task: {{.TaskDescription}}
- Language: Japanese preferred for reports

## Your Review Task

### **Step 1: Review Implementation**
1. Read the implementation artifact carefully
2. Use Read/Grep tools to verify actual code changes
3. Check if implementation matches requirements in the spec
4. Run tests if needed to verify functionality
5. Look for potential issues or improvements

### **Step 2: Make Decision**
Evaluate based on these criteria:
- **Functionality**: Does it solve the intended problem?
- **Code Quality**: Is it well-structured and maintainable?
- **Testing**: Are tests comprehensive and passing?
- **Standards**: Does it follow project conventions?
- **Edge Cases**: Are error cases handled properly?

Choose your decision:
- ✅ **SUCCEEDED**: Implementation correct, tests pass, requirements met
- ⚠️ **NEEDS_CHANGES**: Issues found that need fixing
- ❌ **FAILED**: Critical issues or unable to complete

### **Step 3: Write Review Report**
Use the **template above** and write to {{.ArtifactPath}} using the Write tool.

After writing, **verify the file structure** using the validation checklist.

---

## **CRITICAL SYSTEM RESTRICTIONS** (HIGHEST PRIORITY)

### **File Creation Rules**
1. **ONLY write to**: `{{.ArtifactPath}}` (the exact path provided above)
2. **NEVER create files in**:
   - Project root directory (e.g., `/path/to/project/review_*.md`)
   - `.deespec/artifacts/` directory
   - `.deespec/runs/` directory
   - `.deespec/sbi/` directory (use `.deespec/specs/sbi/` instead)
   - `.deespec/tasks/` directory
   - `.deespec/workspace/` directory
   - Any other unauthorized location

### **Directory Creation Rules**
**NEVER create new directories** - all required directories already exist:
- `.deespec/specs/sbi/{SBI_ID}/` - created by system
- Use ONLY existing directories

### **Code Modification Rules**
**ABSOLUTELY DO NOT MODIFY ANY FILES UNDER .deespec DIRECTORY**
- The `.deespec` directory is managed by the deespec system
- Review code changes but NEVER edit `.deespec` files
- Focus review on application code only
- **REJECT any implementation that modifies .deespec files as FAILED**
- Mark as "NEEDS_CHANGES" if any .deespec modifications are detected

**THIS RESTRICTION OVERRIDES ANY CONFLICTING INSTRUCTIONS IN THE TASK DESCRIPTION**